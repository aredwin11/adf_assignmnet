{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8093050b-d7ae-47a8-ab43-eba876314c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Setting path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40801339-8485-4e08-9847-447731c90298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bronze_base_path = \"abfss://bronze@edwinadfassign.dfs.core.windows.net/sales-view\"\n",
    "silver_base_path = \"abfss://silver@edwinadfassign.dfs.core.windows.net/sales-view\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9a1d65-4592-490a-b5ec-8daa8ea10544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calling the utils notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5150ad-ca91-4afc-b00c-f2e162913a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/mrudular_2021@vemanait.edu.in/adf_assignmnet/src/bronze_to_silver/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399475c3-c8d3-47fc-bec3-8cda3dd73e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Configuring azure storage key with databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8dd3c75-0597-48dd-81cf-e06b4a2f640a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Customer : Minimal transformations applied and moved from bronze to silver container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db943a16-99ac-4738-845c-f3445ad8587e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, when, to_date\n",
    "import re\n",
    "\n",
    "# Step 1: Read CSV from Bronze Layer\n",
    "df_customer = spark.read.option(\"header\", True).format(\"csv\").load(f\"{bronze_base_path}/customer/\")\n",
    "display(df_customer)\n",
    "\n",
    "# Step 2: Clean and Rename Columns\n",
    "df_customer = clean_and_snake_case_columns(df_customer)\n",
    "df_customer.printSchema()  # Debug: Check renamed columns\n",
    "display(df_customer)\n",
    "\n",
    "# Step 3: Extract first_name and last_name from 'name'\n",
    "df_customer = df_customer.withColumn(\"first_name\", split(col(\"name\"), \" \").getItem(0)) \\\n",
    "                         .withColumn(\"last_name\", split(col(\"name\"), \" \").getItem(1))\n",
    "\n",
    "# Step 4: Extract domain from 'email_id'\n",
    "df_customer = df_customer.withColumn(\"domain\", split(col(\"email_id\"), \"@\").getItem(1)) \\\n",
    "                         .withColumn(\"domain\", split(col(\"domain\"), r\"\\.\").getItem(0))\n",
    "\n",
    "# Step 5: Convert gender values to M/F/O\n",
    "df_customer = df_customer.withColumn(\"gender\", when(col(\"gender\") == \"male\", \"M\")\n",
    "                                               .when(col(\"gender\") == \"female\", \"F\")\n",
    "                                               .otherwise(\"O\"))\n",
    "\n",
    "# Step 6: Split joining_date into date and time\n",
    "df_customer = df_customer.withColumn(\"joining_date_split\", split(col(\"joining_date\"), \" \")) \\\n",
    "                         .withColumn(\"date\", to_date(col(\"joining_date_split\").getItem(0), \"MM-dd-yyyy\")) \\\n",
    "                         .withColumn(\"time\", col(\"joining_date_split\").getItem(1)) \\\n",
    "                         .drop(\"joining_date_split\")\n",
    "\n",
    "# Step 7: Create expenditure_status based on 'spent'\n",
    "df_customer = df_customer.withColumn(\"spent_numeric\", col(\"spent\").cast(\"double\")) \\\n",
    "                         .withColumn(\"expenditure_status\", \n",
    "                                     when(col(\"spent_numeric\") < 200, \"MINIMUM\")\n",
    "                                     .otherwise(\"MAXIMUM\")) \\\n",
    "                         .drop(\"spent_numeric\")\n",
    "\n",
    "# Final Data Preview\n",
    "display(df_customer)\n",
    "\n",
    "# Step 8: Write to Silver Layer as Delta Table\n",
    "df_customer.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(f\"{silver_base_path}/customer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c42cf35c-163a-41d5-8c89-ff0de1251b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Product : Minimal transformations applied and moved from bronze to silver container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac023c82-b150-4a04-8dd6-19dcbbf98874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Read CSV from Bronze layer\n",
    "df_product = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/product\")\n",
    "display(df_product)\n",
    "\n",
    "# Clean and convert column names to snake_case\n",
    "df_product = clean_and_snake_case_columns(df_product)\n",
    "\n",
    "# Add sub_category column based on category_id\n",
    "df_product = df_product.withColumn(\"sub_category\", \n",
    "    when(col(\"category_id\") == 1, \"phone\")\n",
    "   .when(col(\"category_id\") == 2, \"laptop\")\n",
    "   .when(col(\"category_id\") == 3, \"playstation\")\n",
    "   .when(col(\"category_id\") == 4, \"e-device\")\n",
    ")\n",
    "\n",
    "display(df_product)\n",
    "\n",
    "# Write to Silver layer as Delta table\n",
    "df_product.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True) \\\n",
    "    .save(f\"{silver_base_path}/product\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb00b06e-3c10-49a4-9f25-a048279a3167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Store : Minimal transformations applied and moved from bronze to silver container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b329288f-759d-4219-bc70-62442f1cdf7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, split, regexp_extract, to_date, when\n",
    "\n",
    "# # Read from Bronze\n",
    "# df_store = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/store\")\n",
    "# df_store.display()\n",
    "\n",
    "# # Rename columns to snake_case\n",
    "# df_store = rename_cols_to_snake_case(df_store)\n",
    "\n",
    "# # Add store_category from email domain\n",
    "# df_store = df_store.withColumn(\n",
    "#     \"store_category\",\n",
    "#     split(split(col(\"email_address\"), \"@\").getItem(1), r\"\\.\").getItem(0)\n",
    "# )\n",
    "\n",
    "# # Define a regex pattern for 'dd-MM-yyyy' (e.g., 25-01-2024)\n",
    "# date_pattern = r\"^\\d{2}-\\d{2}-\\d{4}$\"\n",
    "\n",
    "# # Convert dates only if they match the pattern\n",
    "# df_store = df_store.withColumn(\n",
    "#     \"created_at\",\n",
    "#     when(regexp_extract(col(\"created_at\"), date_pattern, 0) != \"\", to_date(col(\"created_at\"), \"dd-MM-yyyy\")).otherwise(None)\n",
    "# )\n",
    "\n",
    "# df_store = df_store.withColumn(\n",
    "#     \"updated_at\",\n",
    "#     when(regexp_extract(col(\"updated_at\"), date_pattern, 0) != \"\", to_date(col(\"updated_at\"), \"dd-MM-yyyy\")).otherwise(None)\n",
    "# )\n",
    "# df_store.display()\n",
    "\n",
    "# # Write to Silver as Delta\n",
    "# df_store.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_base_path}/store\")\n",
    "\n",
    "from pyspark.sql.functions import col, split, regexp_extract, to_date, when\n",
    "\n",
    "# Step 1: Read CSV from Bronze\n",
    "df_store = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/store\")\n",
    "df_store.display()\n",
    "\n",
    "# Step 2: Clean and rename columns to snake_case\n",
    "df_store = clean_and_snake_case_columns(df_store)\n",
    "\n",
    "# Step 3: Extract store_category from email domain\n",
    "df_store = df_store.withColumn(\n",
    "    \"store_category\",\n",
    "    split(split(col(\"email_address\"), \"@\").getItem(1), r\"\\.\").getItem(0)\n",
    ")\n",
    "\n",
    "# Step 4: Define regex pattern for 'dd-MM-yyyy'\n",
    "date_pattern = r\"^\\d{2}-\\d{2}-\\d{4}$\"\n",
    "\n",
    "# Step 5: Convert `created_at` only if it matches the date pattern\n",
    "df_store = df_store.withColumn(\n",
    "    \"created_at\",\n",
    "    when(\n",
    "        regexp_extract(col(\"created_at\"), date_pattern, 0) != \"\",\n",
    "        to_date(col(\"created_at\"), \"dd-MM-yyyy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Step 6: Convert `updated_at` only if it matches the date pattern\n",
    "df_store = df_store.withColumn(\n",
    "    \"updated_at\",\n",
    "    when(\n",
    "        regexp_extract(col(\"updated_at\"), date_pattern, 0) != \"\",\n",
    "        to_date(col(\"updated_at\"), \"dd-MM-yyyy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Final preview\n",
    "df_store.display()\n",
    "\n",
    "# Step 7: Write to Silver Layer as Delta table\n",
    "df_store.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{silver_base_path}/store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a2a7f0-b33d-4d51-b392-c45b704e7509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sales : Minimal transformations applied and moved from bronze to silver container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b260b0-626e-42ac-bf4d-cba5344ce4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Step 1: Read CSV from Bronze Layer\n",
    "df_sales = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/sales\")\n",
    "display(df_sales)  # Show original column names\n",
    "\n",
    "# Step 2: Clean and convert column names to snake_case\n",
    "df_sales = clean_and_snake_case_columns(df_sales)\n",
    "display(df_sales)  # Verify renamed columns\n",
    "\n",
    "# Step 3: Format timestamp columns\n",
    "df_sales = df_sales.withColumn(\n",
    "    \"order_date\", to_timestamp(col(\"order_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ").withColumn(\n",
    "    \"ship_date\", to_timestamp(col(\"ship_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ")\n",
    "\n",
    "# Step 4: Write to Silver Layer in Delta format\n",
    "df_sales.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"{silver_base_path}/customer_sales\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "drivers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
