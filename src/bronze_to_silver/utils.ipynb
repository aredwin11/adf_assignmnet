{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac26244e-c9d4-41b6-8e11-e87d99850d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Utility: Convert camelCase or PascalCase to snake_case\n",
    "def to_snake_case(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert camelCase, PascalCase, or mixed-case strings to snake_case.\n",
    "    Handles common acronyms like 'ID' → 'id' correctly.\n",
    "    \"\"\"\n",
    "    name = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name)  # Insert underscore between camelCase\n",
    "    name = re.sub(r\"([A-Z]+)([A-Z][a-z])\", r\"\\1_\\2\", name)  # Handle acronyms like IDNumber → ID_Number\n",
    "    return name.lower()\n",
    "\n",
    "# Utility: Clean and rename all column names to snake_case using df.toDF()\n",
    "def clean_and_snake_case_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Clean unwanted characters and convert all column names to snake_case.\n",
    "    \"\"\"\n",
    "    cleaned_cols = []\n",
    "    for col_name in df.columns:\n",
    "        # Remove unwanted characters and normalize spacing\n",
    "        cleaned_name = re.sub(r\"[ (){};\\n\\t=]\", \"\", col_name).strip().replace(\" \", \"_\")\n",
    "        # Convert to snake_case\n",
    "        snake_case_name = to_snake_case(cleaned_name)\n",
    "        cleaned_cols.append(snake_case_name)\n",
    "    return df.toDF(*cleaned_cols)\n",
    "\n",
    "# Read delta table and clean + snake_case the column names\n",
    "def read_delta_with_snake_case(spark, path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read a Delta table and return DataFrame with cleaned snake_case column names.\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"delta\").load(path)\n",
    "    return clean_and_snake_case_columns(df)\n",
    "\n",
    "# Join store and product on store_id\n",
    "def get_store_product_data(product_df: DataFrame, store_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Join product_df and store_df on 'store_id' using inner join.\n",
    "    \"\"\"\n",
    "    return store_df.join(product_df, on=\"store_id\", how=\"inner\")\n",
    "\n",
    "# Join sales with enriched store-product data on product_id\n",
    "def enrich_sales_with_store_product(sales_df: DataFrame, store_product_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Join sales_df and store_product_df on 'product_id' using inner join.\n",
    "    \"\"\"\n",
    "    return sales_df.join(store_product_df, on=\"product_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "84d9302b-b033-47a5-98e6-c7ece6bb8442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType\n",
    "\n",
    "# # UDF to convert camelCase to snake_case\n",
    "# @udf(StringType())\n",
    "# def to_snake_case(col_name):\n",
    "#     \"\"\"\n",
    "#     Convert a single column name from camelCase to snake_case using regex.\n",
    "#     \"\"\"\n",
    "#     s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', col_name)\n",
    "#     return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "# # Rename all columns of a DataFrame to snake_case\n",
    "# def rename_cols_to_snake_case(df):\n",
    "#     \"\"\"\n",
    "#     Rename all column names in the DataFrame to snake_case.\n",
    "#     \"\"\"\n",
    "#     for col_name in df.columns:\n",
    "#         new_col = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', col_name)\n",
    "#         new_col = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', new_col).lower()\n",
    "#         df = df.withColumnRenamed(col_name, new_col)\n",
    "#     return df\n",
    "\n",
    "# # Read delta table and rename columns to snake_case\n",
    "# def read_delta_with_snake_case(spark, path):\n",
    "#     \"\"\"\n",
    "#     Read a Delta table and rename all columns to snake_case.\n",
    "#     \"\"\"\n",
    "#     df = spark.read.format(\"delta\").load(path)\n",
    "#     return rename_cols_to_snake_case(df)\n",
    "\n",
    "# # Join store and product on store_id\n",
    "# def get_store_product_data(product_df, store_df):\n",
    "#     \"\"\"\n",
    "#     Join product_df and store_df on 'store_id' using inner join.\n",
    "#     \"\"\"\n",
    "#     return store_df.join(product_df, on=\"store_id\", how=\"inner\")\n",
    "\n",
    "# # Join sales with enriched store-product data on product_id\n",
    "# def enrich_sales_with_store_product(sales_df, store_product_df):\n",
    "#     \"\"\"\n",
    "#     Join sales_df and store_product_df on 'product_id' using inner join.\n",
    "#     \"\"\"\n",
    "#     return sales_df.join(store_product_df, on=\"product_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9c5019c0-9bf4-41ef-a007-d6db14a06bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pyspark.sql import DataFrame\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType\n",
    "\n",
    "# # UDF to convert camelCase to snake_case\n",
    "# @udf(StringType())\n",
    "# def to_snake_case(col_name: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Convert a single column name from camelCase to snake_case using regex.\n",
    "#     \"\"\"\n",
    "#     s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', col_name)\n",
    "#     return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "# # Rename all columns of a DataFrame to snake_case\n",
    "# def rename_cols_to_snake_case(df: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Rename all column names in the DataFrame to snake_case.\n",
    "#     \"\"\"\n",
    "#     for col_name in df.columns:\n",
    "#         new_col = to_snake_case(col_name)  # Use the UDF for consistency\n",
    "#         df = df.withColumnRenamed(col_name, new_col)\n",
    "#     return df\n",
    "\n",
    "# # Read delta table and rename columns to snake_case\n",
    "# def read_delta_with_snake_case(spark, path: str) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Read a Delta table and rename all columns to snake_case.\n",
    "#     \"\"\"\n",
    "#     df = spark.read.format(\"delta\").load(path)\n",
    "#     return rename_cols_to_snake_case(df)\n",
    "\n",
    "# # Join store and product on store_id\n",
    "# def get_store_product_data(product_df: DataFrame, store_df: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Join product_df and store_df on 'store_id' using inner join.\n",
    "#     \"\"\"\n",
    "#     return store_df.join(product_df, on=\"store_id\", how=\"inner\")\n",
    "\n",
    "# # Join sales with enriched store-product data on product_id\n",
    "# def enrich_sales_with_store_product(sales_df: DataFrame, store_product_df: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Join sales_df and store_product_df on 'product_id' using inner join.\n",
    "#     \"\"\"\n",
    "#     return sales_df.join(store_product_df, on=\"product_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "86685045-b0c4-4818-868d-a55e77c991ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pyspark.sql import DataFrame\n",
    "# from pyspark.sql.functions import col, split, lower, date_format, when, regexp_extract\n",
    "# from pyspark.sql.types import StringType\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # UDF to convert camelCase to snake_case\n",
    "# def to_snake_case(col_name: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Convert a single column name from camelCase to snake_case using regex.\n",
    "#     \"\"\"\n",
    "#     s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', col_name)\n",
    "#     return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "# # Function to rename columns in a DataFrame to snake_case\n",
    "# def rename_cols_to_snake_case(df: DataFrame) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Rename all column names in the DataFrame to snake_case.\n",
    "#     \"\"\"\n",
    "#     for col_name in df.columns:\n",
    "#         new_col = to_snake_case(col_name)\n",
    "#         df = df.withColumnRenamed(col_name, new_col)\n",
    "#     return df\n",
    "\n",
    "# # Function to transform customer DataFrame\n",
    "# def transform_customer_data(df: DataFrame) -> DataFrame:\n",
    "#     df = rename_cols_to_snake_case(df)\n",
    "    \n",
    "#     # Split Name into first_name and last_name\n",
    "#     df = df.withColumn(\"first_name\", split(col(\"name\"), \" \").getItem(0)) \\\n",
    "#            .withColumn(\"last_name\", split(col(\"name\"), \" \").getItem(1))\n",
    "    \n",
    "#     # Extract domain from email\n",
    "#     df = df.withColumn(\"domain\", regexp_extract(col(\"email\"), r'@(.+)', 1))\n",
    "    \n",
    "#     # Map gender\n",
    "#     df = df.withColumn(\"gender\", when(col(\"gender\") == \"Male\", \"M\").otherwise(\"F\"))\n",
    "    \n",
    "#     # Split Joining date into date and time\n",
    "#     df = df.withColumn(\"date\", date_format(split(col(\"joining_date\"), \" \").getItem(0), \"yyyy-MM-dd\")) \\\n",
    "#            .withColumn(\"time\", split(col(\"joining_date\"), \" \").getItem(1))\n",
    "    \n",
    "#     # Create expenditure-status\n",
    "#     df = df.withColumn(\"expenditure_status\", when(col(\"spent\") < 200, \"MINIMUM\").otherwise(\"MAXIMUM\"))\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Function to transform product DataFrame\n",
    "# def transform_product_data(df: DataFrame) -> DataFrame:\n",
    "#     df = rename_cols_to_snake_case(df)\n",
    "    \n",
    "#     # Create sub_category based on category_id\n",
    "#     df = df.withColumn(\"sub_category\", \n",
    "#                        when(col(\"category_id\") == 1, \"phone\")\n",
    "#                        .when(col(\"category_id\") == 2, \"laptop\")\n",
    "#                        .when(col(\"category_id\") == 3, \"playstation\")\n",
    "#                        .when(col(\"category_id\") == 4, \"e-device\"))\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Function to transform store DataFrame\n",
    "# def transform_store_data(df: DataFrame) -> DataFrame:\n",
    "#     df = rename_cols_to_snake_case(df)\n",
    "    \n",
    "#     # Extract store category from email\n",
    "#     df = df.withColumn(\"store_category\", regexp_extract(col(\"email\"), r'@(.+?)\\.', 1))\n",
    "    \n",
    "#     # Format created_at and updated_at\n",
    "#     df = df.withColumn(\"created_at\", date_format(col(\"created_at\"), \"yyyy-MM-dd\")) \\\n",
    "#            .withColumn(\"updated_at\", date_format(col(\"updated_at\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Function to transform sales DataFrame\n",
    "# def transform_sales_data(df: DataFrame) -> DataFrame:\n",
    "#     df = rename_cols_to_snake_case(df)\n",
    "#     return df\n",
    "\n",
    "# # Function to create the final gold layer DataFrame\n",
    "# def create_gold_layer(sales_df: DataFrame, product_df: DataFrame, store_df: DataFrame) -> DataFrame:\n",
    "#     # Join sales with product and store data\n",
    "#     joined_df = sales_df.join(product_df, on=\"product_id\", how=\"inner\") \\\n",
    "#                          .join(store_df, on=\"store_id\", how=\"inner\")\n",
    "    \n",
    "#     # Select required columns for the gold layer\n",
    "#     gold_df = joined_df.select(\n",
    "#         col(\"order_date\"),\n",
    "#         col(\"category\"),\n",
    "#         col(\"city\"),\n",
    "#         col(\"customer_id\"),\n",
    "#         col(\"order_id\"),\n",
    "#         col(\"product_id\"),\n",
    "#         col(\"profit\"),\n",
    "#         col(\"region\"),\n",
    "#         col(\"sales\"),\n",
    "#         col(\"segment\"),\n",
    "#         col(\"ship_date\"),\n",
    "#         col(\"ship_mode\"),\n",
    "#         col(\"latitude\"),\n",
    "#         col(\"longitude\"),\n",
    "#         col(\"store_name\"),\n",
    "#         col(\"location\"),\n",
    "#         col(\"manager_name\"),\n",
    "#         col(\"product_name\"),\n",
    "#         col(\"price\"),\n",
    "#         col(\"stock_quantity\"),\n",
    "#         col(\"image_url\")\n",
    "#     )\n",
    "    \n",
    "#     return gold_df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
