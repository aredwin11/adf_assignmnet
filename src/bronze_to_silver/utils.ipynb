{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac26244e-c9d4-41b6-8e11-e87d99850d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Utility: Convert camelCase or PascalCase to snake_case\n",
    "def to_snake_case(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert camelCase, PascalCase, or mixed-case strings to snake_case.\n",
    "    Handles common acronyms like 'ID' → 'id' correctly.\n",
    "    \"\"\"\n",
    "    name = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name)  # Insert underscore between camelCase\n",
    "    name = re.sub(r\"([A-Z]+)([A-Z][a-z])\", r\"\\1_\\2\", name)  # Handle acronyms like IDNumber → ID_Number\n",
    "    return name.lower()\n",
    "\n",
    "# Utility: Clean and rename all column names to snake_case using df.toDF()\n",
    "def clean_and_snake_case_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Clean unwanted characters and convert all column names to snake_case.\n",
    "    \"\"\"\n",
    "    cleaned_cols = []\n",
    "    for col_name in df.columns:\n",
    "        # Remove unwanted characters and normalize spacing\n",
    "        cleaned_name = re.sub(r\"[ (){};\\n\\t=]\", \"\", col_name).strip().replace(\" \", \"_\")\n",
    "        # Convert to snake_case\n",
    "        snake_case_name = to_snake_case(cleaned_name)\n",
    "        cleaned_cols.append(snake_case_name)\n",
    "    return df.toDF(*cleaned_cols)\n",
    "\n",
    "# Read delta table and clean + snake_case the column names\n",
    "def read_delta_with_snake_case(spark, path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read a Delta table and return DataFrame with cleaned snake_case column names.\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"delta\").load(path)\n",
    "    return clean_and_snake_case_columns(df)\n",
    "\n",
    "# Join store and product on store_id\n",
    "def get_store_product_data(product_df: DataFrame, store_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Join product_df and store_df on 'store_id' using inner join.\n",
    "    \"\"\"\n",
    "    return store_df.join(product_df, on=\"store_id\", how=\"inner\")\n",
    "\n",
    "# Join sales with enriched store-product data on product_id\n",
    "def enrich_sales_with_store_product(sales_df: DataFrame, store_product_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Join sales_df and store_product_df on 'product_id' using inner join.\n",
    "    \"\"\"\n",
    "    return sales_df.join(store_product_df, on=\"product_id\", how=\"inner\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
